{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = action \n",
    "1 = perpetuator\n",
    "2 = victim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.45)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (2024.6.2)\n",
      "Requirement already satisfied: idna==3.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (3.7)\n",
      "Requirement already satisfied: cycler in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (1.4.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (3.8.4)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (1.26.4)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (10.3.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (0.21.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (2.32.2)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (4.66.4)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in c:\\users\\hp\\anaconda3\\lib\\site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->roboflow) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->roboflow) (4.51.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->roboflow) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->roboflow) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->roboflow) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (8.0.196)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.22.2 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (1.26.3)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (1.14.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (2.4.1+cu124)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (0.19.1+cu124)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (4.66.5)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from ultralytics) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (70.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\desktop\\fall\\.venv\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in final-1 to yolov8:: 100%|██████████| 11172/11172 [00:02<00:00, 3737.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to final-1 in yolov8:: 100%|██████████| 628/628 [00:00<00:00, 4077.01it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"I6Dg08bETrwTKkOyF8U9\")\n",
    "project = rf.workspace(\"sreev-rln23\").project(\"final-b89dj\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in cctv-1 to yolov8:: 100%|██████████| 51038/51038 [00:07<00:00, 6624.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to cctv-1 in yolov8:: 100%|██████████| 2804/2804 [00:01<00:00, 1884.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"I6Dg08bETrwTKkOyF8U9\")\n",
    "project = rf.workspace(\"starklabs\").project(\"cctv-lch7j\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "label_dir = 'C:\\\\Users\\\\HP\\\\Desktop\\\\fall\\\\harass1\\\\train\\\\labels'  # Update this with your labels directory\n",
    "\n",
    "for label_file in os.listdir(label_dir):\n",
    "    if label_file.endswith('.txt'):\n",
    "        with open(os.path.join(label_dir, label_file)) as f:\n",
    "            for line in f:\n",
    "                parts = line.split()\n",
    "                if parts:  # Check if line is not empty\n",
    "                    try:\n",
    "                        # Attempt to convert the first part to int\n",
    "                        class_id = int(parts[0])\n",
    "                    except ValueError:\n",
    "                        print(f\"Invalid class id in {label_file}: '{parts[0]}' in line: '{line.strip()}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\fall\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:567: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n",
      "New https://pypi.org/project/ultralytics/8.3.3 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.196  Python-3.12.6 torch-2.4.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4050 Laptop GPU, 6140MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:\\Users\\HP\\Desktop\\fall\\harass1\\dataharass1.yaml, epochs=50, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=cuda, workers=4, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train16\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2117209  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11136761 parameters, 11136745 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "c:\\Users\\HP\\Desktop\\fall\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:567: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n",
      "c:\\Users\\HP\\Desktop\\fall\\.venv\\Lib\\site-packages\\ultralytics\\utils\\checks.py:558: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(True):\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "c:\\Users\\HP\\Desktop\\fall\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:238: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = amp.GradScaler(enabled=self.amp)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\HP\\Desktop\\fall\\harass1\\train\\labels.cache... 927 images, 0 backgrounds, 0 corrupt: 100%|██████████| 927/927 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\HP\\Desktop\\fall\\harass1\\valid\\labels.cache... 244 images, 0 backgrounds, 0 corrupt: 100%|██████████| 244/244 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train16\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train16\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/50      1.18G      1.302      1.967      1.668         15        640: 100%|██████████| 232/232 [00:29<00:00,  7.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:03<00:00,  8.27it/s]\n",
      "                   all        244        539      0.615      0.356      0.301      0.142\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/50      1.23G      1.363      1.892      1.704         13        640: 100%|██████████| 232/232 [00:34<00:00,  6.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.23it/s]\n",
      "                   all        244        539      0.179      0.295      0.157     0.0591\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/50      1.23G      1.403      2.014      1.749         18        640: 100%|██████████| 232/232 [00:45<00:00,  5.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.35it/s]\n",
      "                   all        244        539      0.586      0.291      0.232     0.0937\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/50      1.23G      1.404      1.985      1.741         18        640: 100%|██████████| 232/232 [00:45<00:00,  5.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.32it/s]\n",
      "                   all        244        539      0.484      0.319      0.159     0.0575\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/50      1.23G      1.385      1.965      1.728         15        640: 100%|██████████| 232/232 [00:43<00:00,  5.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.77it/s]\n",
      "                   all        244        539      0.608      0.319      0.246     0.0988\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/50      1.25G      1.351      1.895      1.692         13        640: 100%|██████████| 232/232 [00:43<00:00,  5.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.79it/s]\n",
      "                   all        244        539      0.273      0.347      0.281      0.123\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/50      1.24G      1.314      1.878      1.685         14        640: 100%|██████████| 232/232 [00:43<00:00,  5.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.73it/s]\n",
      "                   all        244        539      0.686      0.357       0.37      0.167\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/50      1.23G      1.289        1.8      1.648         18        640: 100%|██████████| 232/232 [00:44<00:00,  5.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.73it/s]\n",
      "                   all        244        539      0.278      0.409      0.339      0.152\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/50      1.24G      1.249      1.808      1.626         13        640: 100%|██████████| 232/232 [00:46<00:00,  5.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.58it/s]\n",
      "                   all        244        539      0.698       0.41        0.4      0.211\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/50      1.23G      1.228      1.725      1.612          7        640: 100%|██████████| 232/232 [00:43<00:00,  5.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.35it/s]\n",
      "                   all        244        539      0.668      0.407      0.415      0.208\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/50      1.23G      1.209      1.708      1.595         11        640: 100%|██████████| 232/232 [00:44<00:00,  5.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.63it/s]\n",
      "                   all        244        539      0.737      0.428      0.444      0.243\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/50      1.23G      1.136      1.643      1.542         13        640: 100%|██████████| 232/232 [00:44<00:00,  5.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.66it/s]\n",
      "                   all        244        539      0.798      0.395      0.477      0.257\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/50      1.23G       1.17      1.673      1.573         19        640: 100%|██████████| 232/232 [00:44<00:00,  5.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.42it/s]\n",
      "                   all        244        539       0.77      0.426       0.47      0.259\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/50      1.23G      1.135      1.578      1.541         13        640: 100%|██████████| 232/232 [00:43<00:00,  5.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:06<00:00,  5.03it/s]\n",
      "                   all        244        539      0.796      0.419      0.473      0.274\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/50      1.23G       1.11      1.537      1.522         18        640: 100%|██████████| 232/232 [00:44<00:00,  5.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.40it/s]\n",
      "                   all        244        539      0.782      0.437      0.477      0.253\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/50      1.24G      1.062      1.489      1.486         20        640: 100%|██████████| 232/232 [00:43<00:00,  5.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.72it/s]\n",
      "                   all        244        539      0.801      0.488       0.55      0.343\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/50      1.23G      1.059      1.464      1.492         17        640: 100%|██████████| 232/232 [00:43<00:00,  5.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.78it/s]\n",
      "                   all        244        539      0.828      0.484      0.557      0.353\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/50      1.23G      1.018      1.409      1.454         13        640: 100%|██████████| 232/232 [00:43<00:00,  5.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.63it/s]\n",
      "                   all        244        539      0.858      0.447      0.562      0.352\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/50      1.23G      1.024      1.376      1.445         16        640: 100%|██████████| 232/232 [00:42<00:00,  5.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.74it/s]\n",
      "                   all        244        539      0.721      0.523      0.556      0.355\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/50      1.23G     0.9909      1.361      1.435         13        640: 100%|██████████| 232/232 [00:45<00:00,  5.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.73it/s]\n",
      "                   all        244        539      0.825      0.527      0.613      0.393\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/50      1.23G     0.9958      1.333      1.443         22        640: 100%|██████████| 232/232 [00:44<00:00,  5.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.48it/s]\n",
      "                   all        244        539      0.883      0.452      0.596      0.385\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/50      1.23G     0.9399       1.26      1.407         29        640: 100%|██████████| 232/232 [00:43<00:00,  5.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.59it/s]\n",
      "                   all        244        539      0.555      0.593      0.615      0.401\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/50      1.25G     0.9249       1.24      1.399         13        640: 100%|██████████| 232/232 [00:44<00:00,  5.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.25it/s]\n",
      "                   all        244        539      0.531      0.605      0.599      0.399\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/50      1.25G     0.9432      1.265      1.399          6        640: 100%|██████████| 232/232 [00:42<00:00,  5.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.54it/s]\n",
      "                   all        244        539        0.6      0.677      0.629      0.427\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/50      1.23G     0.8933      1.188      1.366         13        640: 100%|██████████| 232/232 [00:43<00:00,  5.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.47it/s]\n",
      "                   all        244        539      0.595      0.606      0.631      0.453\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/50      1.24G     0.8621      1.119      1.341         14        640: 100%|██████████| 232/232 [00:43<00:00,  5.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.68it/s]\n",
      "                   all        244        539      0.657      0.613       0.67      0.486\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/50      1.23G     0.8824      1.147      1.362         12        640: 100%|██████████| 232/232 [00:43<00:00,  5.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:03<00:00,  7.92it/s]\n",
      "                   all        244        539      0.636      0.603      0.681      0.496\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/50      1.23G     0.8474      1.098      1.331         20        640: 100%|██████████| 232/232 [00:26<00:00,  8.64it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:03<00:00,  9.31it/s]\n",
      "                   all        244        539      0.708      0.626      0.675      0.492\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/50      1.23G     0.8321      1.087      1.328         15        640: 100%|██████████| 232/232 [00:26<00:00,  8.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:03<00:00,  9.33it/s]\n",
      "                   all        244        539      0.876      0.565      0.684      0.499\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/50      1.23G     0.8111      1.042       1.31         18        640: 100%|██████████| 232/232 [00:25<00:00,  8.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:03<00:00,  7.77it/s]\n",
      "                   all        244        539      0.669      0.681      0.708      0.512\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      31/50      1.23G     0.8167      1.047      1.316         20        640: 100%|██████████| 232/232 [00:29<00:00,  7.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:03<00:00,  7.85it/s]\n",
      "                   all        244        539      0.674      0.668      0.708      0.528\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      32/50      1.22G     0.7904     0.9953      1.298         12        640: 100%|██████████| 232/232 [00:29<00:00,  7.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:03<00:00,  7.92it/s]\n",
      "                   all        244        539      0.782      0.682      0.745      0.555\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      33/50      1.25G     0.7893     0.9759      1.284         11        640: 100%|██████████| 232/232 [00:30<00:00,  7.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:03<00:00,  8.25it/s]\n",
      "                   all        244        539       0.83      0.694      0.775      0.592\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      34/50      1.23G     0.7326     0.9349      1.243         15        640: 100%|██████████| 232/232 [00:29<00:00,  7.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:03<00:00,  7.96it/s]\n",
      "                   all        244        539      0.623      0.756      0.716      0.554\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      35/50      1.24G       0.73     0.9142      1.248         17        640: 100%|██████████| 232/232 [00:30<00:00,  7.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:04<00:00,  7.63it/s]\n",
      "                   all        244        539      0.904      0.674      0.779      0.584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      36/50      1.23G     0.7366        0.9      1.258         18        640: 100%|██████████| 232/232 [00:29<00:00,  7.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:04<00:00,  7.57it/s]\n",
      "                   all        244        539       0.79      0.781      0.824      0.619\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      37/50      1.24G     0.7231     0.9114      1.237         15        640: 100%|██████████| 232/232 [00:32<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:03<00:00,  8.16it/s]\n",
      "                   all        244        539      0.811      0.752      0.801      0.598\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      38/50      1.23G     0.7016     0.8799      1.236         18        640: 100%|██████████| 232/232 [00:29<00:00,  7.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:03<00:00,  8.16it/s]\n",
      "                   all        244        539       0.84      0.758      0.819      0.622\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      39/50      1.23G     0.6802     0.8409      1.212          8        640: 100%|██████████| 232/232 [00:41<00:00,  5.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:06<00:00,  4.96it/s]\n",
      "                   all        244        539      0.862      0.765      0.818      0.634\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      40/50      1.23G     0.6665     0.8288      1.212         11        640: 100%|██████████| 232/232 [00:30<00:00,  7.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:04<00:00,  7.42it/s]\n",
      "                   all        244        539      0.836      0.764      0.839      0.663\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      41/50      1.25G     0.5701     0.6227      1.194          7        640: 100%|██████████| 232/232 [00:21<00:00, 10.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:02<00:00, 12.99it/s]\n",
      "                   all        244        539      0.913      0.792      0.888       0.68\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      42/50      1.22G     0.5487     0.5635      1.181          7        640: 100%|██████████| 232/232 [00:18<00:00, 12.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:02<00:00, 12.97it/s]\n",
      "                   all        244        539      0.895      0.799      0.884      0.699\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      43/50      1.23G     0.5088     0.5039      1.136          5        640: 100%|██████████| 232/232 [00:19<00:00, 12.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:02<00:00, 12.94it/s]\n",
      "                   all        244        539      0.829      0.846      0.887      0.706\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      44/50      1.23G      0.475     0.4773      1.112          6        640: 100%|██████████| 232/232 [00:19<00:00, 11.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:02<00:00, 11.58it/s]\n",
      "                   all        244        539      0.894       0.86      0.903      0.728\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      45/50      1.23G     0.4757     0.4683      1.113          7        640: 100%|██████████| 232/232 [00:29<00:00,  7.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  5.35it/s]\n",
      "                   all        244        539      0.894      0.849      0.916      0.735\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      46/50      1.24G     0.4488     0.4368      1.083          6        640: 100%|██████████| 232/232 [00:42<00:00,  5.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:07<00:00,  3.97it/s]\n",
      "                   all        244        539      0.884      0.862      0.902       0.74\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      47/50      1.24G     0.4242     0.4325      1.077          9        640: 100%|██████████| 232/232 [00:55<00:00,  4.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:08<00:00,  3.79it/s]\n",
      "                   all        244        539      0.904      0.881      0.914      0.756\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      48/50      1.23G     0.4129     0.3994      1.059          5        640: 100%|██████████| 232/232 [01:02<00:00,  3.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:08<00:00,  3.85it/s]\n",
      "                   all        244        539      0.934      0.914      0.952       0.78\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      49/50      1.23G     0.4015     0.3867      1.049          8        640: 100%|██████████| 232/232 [00:59<00:00,  3.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:06<00:00,  4.73it/s]\n",
      "                   all        244        539      0.933      0.885      0.917      0.769\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      50/50      1.23G      0.383      0.374      1.027          5        640: 100%|██████████| 232/232 [01:06<00:00,  3.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:09<00:00,  3.28it/s]\n",
      "                   all        244        539      0.906      0.895      0.932      0.793\n",
      "\n",
      "50 epochs completed in 0.647 hours.\n",
      "Optimizer stripped from runs\\detect\\train16\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train16\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train16\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.196  Python-3.12.6 torch-2.4.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4050 Laptop GPU, 6140MiB)\n",
      "Model summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:08<00:00,  3.50it/s]\n",
      "                   all        244        539      0.908      0.909      0.932      0.794\n",
      "                action        244         31      0.745      0.774      0.815      0.545\n",
      "           perpetrator        244        257      0.984      0.969      0.988      0.919\n",
      "                victim        244        251      0.994      0.984      0.993      0.919\n",
      "Speed: 0.4ms preprocess, 7.3ms inference, 0.0ms loss, 5.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train16\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'YOLO' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Run YOLO training on CPU\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m     14\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mHP\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mfall\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mharass1\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdataharass1.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     20\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8_custom_harass.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\Desktop\\fall\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'YOLO' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Change to your desired working directory\n",
    "os.chdir('C:\\\\Users\\\\HP\\\\Desktop\\\\fall')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "# Run YOLO training on CPU\n",
    "model.train(\n",
    "    data = 'C:\\\\Users\\\\HP\\\\Desktop\\\\fall\\\\harass1\\\\dataharass1.yaml',\n",
    "    epochs = 50,\n",
    "    imgsz = 640,\n",
    "    device = device,\n",
    "    batch = 4,\n",
    "    workers = 4\n",
    ")\n",
    "\n",
    "model.save('yolov8_custom_harass.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.196  Python-3.12.6 torch-2.4.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4050 Laptop GPU, 6140MiB)\n",
      "Model summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\HP\\Desktop\\fall\\harass1\\valid\\labels.cache... 244 images, 0 backgrounds, 0 corrupt: 100%|██████████| 244/244 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 16/16 [00:07<00:00,  2.11it/s]\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x000001ADD8F70CC0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\fall\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"c:\\Users\\HP\\Desktop\\fall\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1435, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
      "                   all        244        539      0.908      0.909      0.932      0.792\n",
      "                action        244         31      0.745      0.774      0.815       0.54\n",
      "           perpetrator        244        257      0.984      0.969      0.988      0.919\n",
      "                victim        244        251      0.994      0.984      0.993      0.918\n",
      "Speed: 0.6ms preprocess, 15.6ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
      "\n",
      "ap_class_index: array([0, 1, 2])\n",
      "box: ultralytics.utils.metrics.Metric object\n",
      "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000001AE4E3C9880>\n",
      "fitness: 0.8062209617056239\n",
      "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
      "maps: array([    0.53988,     0.91865,     0.91823])\n",
      "names: {0: 'action', 1: 'perpetrator', 2: 'victim'}\n",
      "plot: True\n",
      "results_dict: {'metrics/precision(B)': 0.9076019866153051, 'metrics/recall(B)': 0.9090429629125855, 'metrics/mAP50(B)': 0.9319354906514145, 'metrics/mAP50-95(B)': 0.7922526807116471, 'fitness': 0.8062209617056239}\n",
      "save_dir: WindowsPath('runs/detect/val5')\n",
      "speed: {'preprocess': 0.6472741971250441, 'inference': 15.59871142027808, 'loss': 0.0005491444321929431, 'postprocess': 2.56381269361152}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = '0'  # Use first GPU\n",
    "else:\n",
    "    device = 'cpu'  # Fall back to CPU\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "# Load the YOLO model with the best weights\n",
    "model = YOLO('runs/detect/train16/weights/best.pt')\n",
    "\n",
    "# Validate the model using the specified dataset\n",
    "results = model.val(data='C:\\\\Users\\\\HP\\\\Desktop\\\\fall\\\\harass1\\\\dataharass1.yaml', device=device)\n",
    "\n",
    "# Print results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.3 🚀 Python-3.12.6 torch-2.4.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4050 Laptop GPU, 6140MiB)\n",
      "Model summary (fused): 168 layers, 11,126,745 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\n",
      "video 1/1 (frame 1/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 perpetrator, 1 victim, 50.2ms\n",
      "video 1/1 (frame 2/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 perpetrator, 1 victim, 11.2ms\n",
      "video 1/1 (frame 3/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 perpetrator, 1 victim, 6.9ms\n",
      "video 1/1 (frame 4/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 perpetrator, 1 victim, 3.5ms\n",
      "video 1/1 (frame 5/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 perpetrator, 1 victim, 14.9ms\n",
      "video 1/1 (frame 6/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 perpetrator, 1 victim, 13.6ms\n",
      "video 1/1 (frame 7/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 10.7ms\n",
      "video 1/1 (frame 8/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 14.5ms\n",
      "video 1/1 (frame 9/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 9.3ms\n",
      "video 1/1 (frame 10/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 7.2ms\n",
      "video 1/1 (frame 11/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 5.8ms\n",
      "video 1/1 (frame 12/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 1.6ms\n",
      "video 1/1 (frame 13/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 13.7ms\n",
      "video 1/1 (frame 14/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 13.7ms\n",
      "video 1/1 (frame 15/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 13.9ms\n",
      "video 1/1 (frame 16/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 15.3ms\n",
      "video 1/1 (frame 17/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 5.9ms\n",
      "video 1/1 (frame 18/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 perpetrator, 1 victim, 7.8ms\n",
      "video 1/1 (frame 19/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 6.8ms\n",
      "video 1/1 (frame 20/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 10.1ms\n",
      "video 1/1 (frame 21/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 13.4ms\n",
      "video 1/1 (frame 22/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 11.5ms\n",
      "video 1/1 (frame 23/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 3.4ms\n",
      "video 1/1 (frame 24/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 20.4ms\n",
      "video 1/1 (frame 25/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 10.1ms\n",
      "video 1/1 (frame 26/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 7.0ms\n",
      "video 1/1 (frame 27/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 victim, 7.2ms\n",
      "video 1/1 (frame 28/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 2 perpetrators, 1 victim, 6.9ms\n",
      "video 1/1 (frame 29/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 2 perpetrators, 1 victim, 12.4ms\n",
      "video 1/1 (frame 30/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 2 perpetrators, 1 victim, 11.3ms\n",
      "video 1/1 (frame 31/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 2 perpetrators, 1 victim, 13.0ms\n",
      "video 1/1 (frame 32/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 2 perpetrators, 11.5ms\n",
      "video 1/1 (frame 33/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 2 perpetrators, 12.0ms\n",
      "video 1/1 (frame 34/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 2 perpetrators, 1 victim, 15.5ms\n",
      "video 1/1 (frame 35/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 2 perpetrators, 0.0ms\n",
      "video 1/1 (frame 36/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 perpetrator, 14.9ms\n",
      "video 1/1 (frame 37/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 perpetrator, 8.2ms\n",
      "video 1/1 (frame 38/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 perpetrator, 14.1ms\n",
      "video 1/1 (frame 39/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 2 perpetrators, 10.5ms\n",
      "video 1/1 (frame 40/40) C:\\Users\\HP\\Desktop\\fall\\harassDataset\\harass1\\harassment.gif: 480x640 1 perpetrator, 11.1ms\n",
      "Speed: 0.9ms preprocess, 11.3ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict29\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=predict model=runs/detect/train16/weights/best.pt conf=0.25 source='C:\\\\Users\\\\HP\\\\Desktop\\\\fall\\\\harassDataset\\\\harass1\\\\harassment.gif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 perpetrator, 99.2ms\n",
      "Speed: 0.0ms preprocess, 99.2ms inference, 116.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 13.5ms\n",
      "Speed: 8.5ms preprocess, 13.5ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 17.4ms\n",
      "Speed: 0.0ms preprocess, 17.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 12.1ms\n",
      "Speed: 7.2ms preprocess, 12.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 23.6ms\n",
      "Speed: 0.0ms preprocess, 23.6ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 20.6ms\n",
      "Speed: 0.0ms preprocess, 20.6ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 17.0ms\n",
      "Speed: 7.4ms preprocess, 17.0ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 16.9ms\n",
      "Speed: 1.7ms preprocess, 16.9ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 11.5ms\n",
      "Speed: 3.1ms preprocess, 11.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 10.9ms\n",
      "Speed: 2.6ms preprocess, 10.9ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 17.7ms\n",
      "Speed: 3.0ms preprocess, 17.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 12.5ms\n",
      "Speed: 1.7ms preprocess, 12.5ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 14.8ms\n",
      "Speed: 0.0ms preprocess, 14.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 13.8ms\n",
      "Speed: 9.3ms preprocess, 13.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 25.3ms\n",
      "Speed: 6.7ms preprocess, 25.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 25.9ms\n",
      "Speed: 4.7ms preprocess, 25.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 18.1ms\n",
      "Speed: 2.6ms preprocess, 18.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 14.5ms\n",
      "Speed: 0.0ms preprocess, 14.5ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 17.0ms\n",
      "Speed: 0.9ms preprocess, 17.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 21.3ms\n",
      "Speed: 2.7ms preprocess, 21.3ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 15.2ms\n",
      "Speed: 3.0ms preprocess, 15.2ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 13.4ms\n",
      "Speed: 0.0ms preprocess, 13.4ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 13.7ms\n",
      "Speed: 3.1ms preprocess, 13.7ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 23.0ms\n",
      "Speed: 0.0ms preprocess, 23.0ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 8.7ms\n",
      "Speed: 3.6ms preprocess, 8.7ms inference, 10.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 16.5ms\n",
      "Speed: 0.0ms preprocess, 16.5ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 9.7ms\n",
      "Speed: 2.2ms preprocess, 9.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 1 victim, 9.9ms\n",
      "Speed: 2.4ms preprocess, 9.9ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 2 victims, 12.7ms\n",
      "Speed: 6.4ms preprocess, 12.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 2 victims, 22.7ms\n",
      "Speed: 0.0ms preprocess, 22.7ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 10.5ms\n",
      "Speed: 10.0ms preprocess, 10.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 2 victims, 10.8ms\n",
      "Speed: 0.0ms preprocess, 10.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 1 victim, 10.5ms\n",
      "Speed: 2.6ms preprocess, 10.5ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 8.1ms\n",
      "Speed: 3.5ms preprocess, 8.1ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 20.9ms\n",
      "Speed: 0.0ms preprocess, 20.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 13.2ms\n",
      "Speed: 7.4ms preprocess, 13.2ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 21.3ms\n",
      "Speed: 0.0ms preprocess, 21.3ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 24.5ms\n",
      "Speed: 9.2ms preprocess, 24.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 7.2ms\n",
      "Speed: 1.2ms preprocess, 7.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 7.0ms\n",
      "Speed: 6.8ms preprocess, 7.0ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 18.1ms\n",
      "Speed: 2.6ms preprocess, 18.1ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 13.5ms\n",
      "Speed: 7.3ms preprocess, 13.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 13.9ms\n",
      "Speed: 0.0ms preprocess, 13.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 7.0ms\n",
      "Speed: 6.7ms preprocess, 7.0ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 11.1ms\n",
      "Speed: 1.1ms preprocess, 11.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 7.2ms\n",
      "Speed: 6.1ms preprocess, 7.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.3ms\n",
      "Speed: 1.1ms preprocess, 10.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.6ms\n",
      "Speed: 9.9ms preprocess, 10.6ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 6.7ms\n",
      "Speed: 0.5ms preprocess, 6.7ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 6.0ms\n",
      "Speed: 7.5ms preprocess, 6.0ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 14.0ms\n",
      "Speed: 0.0ms preprocess, 14.0ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 2 victims, 27.9ms\n",
      "Speed: 7.1ms preprocess, 27.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 11.5ms\n",
      "Speed: 2.0ms preprocess, 11.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 7.6ms\n",
      "Speed: 4.2ms preprocess, 7.6ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.5ms\n",
      "Speed: 6.7ms preprocess, 10.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 7.0ms\n",
      "Speed: 0.9ms preprocess, 7.0ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 13.3ms\n",
      "Speed: 0.0ms preprocess, 13.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 7.2ms\n",
      "Speed: 0.2ms preprocess, 7.2ms inference, 9.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 13.7ms\n",
      "Speed: 6.2ms preprocess, 13.7ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 7.6ms\n",
      "Speed: 0.0ms preprocess, 7.6ms inference, 8.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 9.8ms\n",
      "Speed: 1.9ms preprocess, 9.8ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 14.7ms\n",
      "Speed: 0.0ms preprocess, 14.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.3ms\n",
      "Speed: 3.6ms preprocess, 10.3ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 13.7ms\n",
      "Speed: 0.0ms preprocess, 13.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 16.1ms\n",
      "Speed: 0.0ms preprocess, 16.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 15.5ms\n",
      "Speed: 0.0ms preprocess, 15.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 14.8ms\n",
      "Speed: 0.0ms preprocess, 14.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.0ms\n",
      "Speed: 10.0ms preprocess, 10.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 14.3ms\n",
      "Speed: 0.0ms preprocess, 14.3ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 12.1ms\n",
      "Speed: 0.0ms preprocess, 12.1ms inference, 8.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.1ms\n",
      "Speed: 0.0ms preprocess, 10.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.1ms\n",
      "Speed: 0.0ms preprocess, 10.1ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 22.1ms\n",
      "Speed: 0.0ms preprocess, 22.1ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 17.6ms\n",
      "Speed: 0.0ms preprocess, 17.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 11.3ms\n",
      "Speed: 0.0ms preprocess, 11.3ms inference, 9.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 12.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 18.6ms\n",
      "Speed: 0.0ms preprocess, 18.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.3ms\n",
      "Speed: 5.0ms preprocess, 10.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 20.0ms\n",
      "Speed: 0.0ms preprocess, 20.0ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 15.9ms\n",
      "Speed: 2.4ms preprocess, 15.9ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 13.9ms\n",
      "Speed: 10.0ms preprocess, 13.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.5ms\n",
      "Speed: 0.0ms preprocess, 10.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 6.1ms\n",
      "Speed: 4.1ms preprocess, 6.1ms inference, 10.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 18.4ms\n",
      "Speed: 0.0ms preprocess, 18.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 20.4ms\n",
      "Speed: 0.0ms preprocess, 20.4ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.2ms\n",
      "Speed: 3.1ms preprocess, 10.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 22.4ms\n",
      "Speed: 0.0ms preprocess, 22.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 11.7ms\n",
      "Speed: 0.0ms preprocess, 11.7ms inference, 8.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 17.4ms\n",
      "Speed: 4.7ms preprocess, 17.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 20.0ms\n",
      "Speed: 0.0ms preprocess, 20.0ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.1ms\n",
      "Speed: 7.6ms preprocess, 10.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 19.5ms\n",
      "Speed: 0.0ms preprocess, 19.5ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.2ms\n",
      "Speed: 6.5ms preprocess, 10.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.1ms\n",
      "Speed: 0.0ms preprocess, 10.1ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 16.1ms\n",
      "Speed: 0.0ms preprocess, 16.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.2ms\n",
      "Speed: 0.0ms preprocess, 10.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 20.0ms\n",
      "Speed: 0.0ms preprocess, 20.0ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 1 victim, 14.7ms\n",
      "Speed: 0.0ms preprocess, 14.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 7.3ms\n",
      "Speed: 6.6ms preprocess, 7.3ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 1 victim, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 18.9ms\n",
      "Speed: 0.0ms preprocess, 18.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 14.7ms\n",
      "Speed: 0.0ms preprocess, 14.7ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 13.0ms\n",
      "Speed: 0.0ms preprocess, 13.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 6.9ms\n",
      "Speed: 6.9ms preprocess, 6.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 22.6ms\n",
      "Speed: 5.7ms preprocess, 22.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 14.2ms\n",
      "Speed: 0.0ms preprocess, 14.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 4.8ms\n",
      "Speed: 2.6ms preprocess, 4.8ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 9.4ms\n",
      "Speed: 13.2ms preprocess, 9.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 14.3ms\n",
      "Speed: 0.0ms preprocess, 14.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 14.6ms\n",
      "Speed: 0.0ms preprocess, 14.6ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 victim, 16.3ms\n",
      "Speed: 0.0ms preprocess, 16.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 7.6ms\n",
      "Speed: 6.4ms preprocess, 7.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 41.6ms\n",
      "Speed: 0.0ms preprocess, 41.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 8.3ms\n",
      "Speed: 1.6ms preprocess, 8.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 6.6ms\n",
      "Speed: 6.9ms preprocess, 6.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 13.8ms\n",
      "Speed: 0.0ms preprocess, 13.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 7.2ms\n",
      "Speed: 2.5ms preprocess, 7.2ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.9ms\n",
      "Speed: 1.1ms preprocess, 10.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 13.3ms\n",
      "Speed: 7.5ms preprocess, 13.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 1 victim, 8.2ms\n",
      "Speed: 0.3ms preprocess, 8.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 8.6ms\n",
      "Speed: 5.3ms preprocess, 8.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 7.1ms\n",
      "Speed: 6.9ms preprocess, 7.1ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 13.6ms\n",
      "Speed: 0.0ms preprocess, 13.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 1 victim, 6.8ms\n",
      "Speed: 7.5ms preprocess, 6.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 7.4ms\n",
      "Speed: 0.0ms preprocess, 7.4ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 13.7ms\n",
      "Speed: 1.6ms preprocess, 13.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 7.2ms\n",
      "Speed: 0.0ms preprocess, 7.2ms inference, 8.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 7.6ms\n",
      "Speed: 3.0ms preprocess, 7.6ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 11.0ms\n",
      "Speed: 0.0ms preprocess, 11.0ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 13.8ms\n",
      "Speed: 5.1ms preprocess, 13.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 17.1ms\n",
      "Speed: 6.6ms preprocess, 17.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 6.7ms\n",
      "Speed: 7.4ms preprocess, 6.7ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 12.0ms\n",
      "Speed: 0.4ms preprocess, 12.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 1 victim, 3.6ms\n",
      "Speed: 3.1ms preprocess, 3.6ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 1 victim, 11.1ms\n",
      "Speed: 2.5ms preprocess, 11.1ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 12.2ms\n",
      "Speed: 5.8ms preprocess, 12.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 15.5ms\n",
      "Speed: 0.0ms preprocess, 15.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 14.1ms\n",
      "Speed: 7.1ms preprocess, 14.1ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 7.4ms\n",
      "Speed: 3.2ms preprocess, 7.4ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 14.2ms\n",
      "Speed: 1.9ms preprocess, 14.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 11.1ms\n",
      "Speed: 5.8ms preprocess, 11.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 15.1ms\n",
      "Speed: 0.0ms preprocess, 15.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 13.5ms\n",
      "Speed: 0.0ms preprocess, 13.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 7.4ms\n",
      "Speed: 5.7ms preprocess, 7.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 8.6ms\n",
      "Speed: 8.2ms preprocess, 8.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 16.9ms\n",
      "Speed: 1.5ms preprocess, 16.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 16.4ms\n",
      "Speed: 1.7ms preprocess, 16.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 16.5ms\n",
      "Speed: 7.3ms preprocess, 16.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 8.4ms\n",
      "Speed: 6.7ms preprocess, 8.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.6ms\n",
      "Speed: 2.5ms preprocess, 10.6ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 3 victims, 9.4ms\n",
      "Speed: 4.6ms preprocess, 9.4ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 8.9ms\n",
      "Speed: 5.1ms preprocess, 8.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 6.8ms\n",
      "Speed: 7.0ms preprocess, 6.8ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 2 victims, 8.1ms\n",
      "Speed: 5.7ms preprocess, 8.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 2 victims, 14.0ms\n",
      "Speed: 0.0ms preprocess, 14.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 2 victims, 12.7ms\n",
      "Speed: 0.0ms preprocess, 12.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 10.1ms\n",
      "Speed: 1.1ms preprocess, 10.1ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 7.4ms\n",
      "Speed: 0.0ms preprocess, 7.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 13.4ms\n",
      "Speed: 1.6ms preprocess, 13.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 13.9ms\n",
      "Speed: 0.0ms preprocess, 13.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 15.4ms\n",
      "Speed: 3.4ms preprocess, 15.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 2 victims, 8.2ms\n",
      "Speed: 7.2ms preprocess, 8.2ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 10.4ms\n",
      "Speed: 2.4ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 7.2ms\n",
      "Speed: 0.0ms preprocess, 7.2ms inference, 11.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 7.0ms\n",
      "Speed: 4.0ms preprocess, 7.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 5.8ms\n",
      "Speed: 3.6ms preprocess, 5.8ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 perpetrators, 13.5ms\n",
      "Speed: 7.0ms preprocess, 13.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 19.4ms\n",
      "Speed: 2.6ms preprocess, 19.4ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 7.6ms\n",
      "Speed: 7.5ms preprocess, 7.6ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 7.7ms\n",
      "Speed: 6.6ms preprocess, 7.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 7.0ms\n",
      "Speed: 2.6ms preprocess, 7.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 7.0ms\n",
      "Speed: 0.4ms preprocess, 7.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 11.2ms\n",
      "Speed: 7.8ms preprocess, 11.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 2 victims, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 7.8ms\n",
      "Speed: 0.0ms preprocess, 7.8ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 7.1ms\n",
      "Speed: 6.7ms preprocess, 7.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 7.3ms\n",
      "Speed: 6.5ms preprocess, 7.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 35.8ms\n",
      "Speed: 0.2ms preprocess, 35.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 9.1ms\n",
      "Speed: 7.4ms preprocess, 9.1ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 9.7ms\n",
      "Speed: 6.8ms preprocess, 9.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.6ms\n",
      "Speed: 1.1ms preprocess, 10.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 10.9ms\n",
      "Speed: 4.1ms preprocess, 10.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 7.3ms\n",
      "Speed: 7.2ms preprocess, 7.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 13.1ms\n",
      "Speed: 7.2ms preprocess, 13.1ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 6.4ms\n",
      "Speed: 7.5ms preprocess, 6.4ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 10.7ms\n",
      "Speed: 5.1ms preprocess, 10.7ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 8.7ms\n",
      "Speed: 5.2ms preprocess, 8.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 25.6ms\n",
      "Speed: 5.1ms preprocess, 25.6ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 25.4ms\n",
      "Speed: 6.6ms preprocess, 25.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 perpetrators, 1 victim, 14.1ms\n",
      "Speed: 0.0ms preprocess, 14.1ms inference, 9.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 2 victims, 11.3ms\n",
      "Speed: 1.9ms preprocess, 11.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 2 victims, 7.1ms\n",
      "Speed: 3.2ms preprocess, 7.1ms inference, 8.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 7.8ms\n",
      "Speed: 6.2ms preprocess, 7.8ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 perpetrator, 1 victim, 8.2ms\n",
      "Speed: 8.1ms preprocess, 8.2ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('C:\\\\Users\\\\HP\\\\Desktop\\\\fall\\\\runs\\\\detect\\\\train16\\\\weights\\\\best.pt')\n",
    "\n",
    "# Create output directory to save frames with Fall Detected\n",
    "output_folder = 'C:\\\\Users\\\\HP\\\\Desktop\\\\fall\\\\output_harass'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "def process_frame(frame, frame_count, confidence_threshold=0.55):\n",
    "    results = model(frame)\n",
    "\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            # Get bounding box coordinates, class label, and confidence score\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            class_id = int(box.cls[0])  # Get the class ID\n",
    "            confidence = box.conf[0]  # Get the confidence score\n",
    "\n",
    "            # Only process if confidence exceeds the threshold\n",
    "            if confidence >= confidence_threshold:\n",
    "                # Define the color based on the class ID and detection confidence\n",
    "                if class_id == 2:  # Class ID for fall detection\n",
    "                    color = (0, 0, 255)  # Red for fall detection\n",
    "                    label = f\"{class_id} ({confidence:.2f})\"\n",
    "                else:\n",
    "                    color = (0, 255, 0)  # Green for other classes\n",
    "                    label = f\"{class_id} ({confidence:.2f})\"\n",
    "\n",
    "                # Draw the bounding box and label\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Function to process image or video\n",
    "def process_input(input_source, confidence_threshold=0.55):\n",
    "    frame_count = 0  # Track the frame number\n",
    "    output_file_path = os.path.join(output_folder, os.path.basename(input_source))\n",
    "\n",
    "    # Check if the input is a video file\n",
    "    if input_source.endswith(('.mp4', '.avi', '.mov', '.gif', '.webm')):\n",
    "        cap = cv2.VideoCapture(input_source)\n",
    "        # Get the video details (frame width, height, FPS)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        # Define the codec and create VideoWriter object\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        output_video = cv2.VideoWriter(output_file_path, fourcc, fps, (width, height))\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Process and display the frame\n",
    "            processed_frame = process_frame(frame, frame_count, confidence_threshold)\n",
    "            output_video.write(processed_frame)  # Write frame to output video file\n",
    "            cv2.imshow('Detections', processed_frame)\n",
    "\n",
    "            # Slow down the video display (e.g., 50 ms per frame)\n",
    "            if cv2.waitKey(50) & 0xFF == ord('q'):  # 50ms = 20 frames per second (fps)\n",
    "                break\n",
    "\n",
    "            frame_count += 1  # Increment frame count\n",
    "\n",
    "        cap.release()\n",
    "        output_video.release()  # Release the VideoWriter object\n",
    "\n",
    "    else:  # Assuming it's an image if not a video\n",
    "        frame = cv2.imread(input_source)\n",
    "        processed_frame = process_frame(frame, frame_count, confidence_threshold)\n",
    "\n",
    "        # Save the processed image in the output folder\n",
    "        output_image_path = os.path.join(output_folder, os.path.basename(input_source))\n",
    "        cv2.imwrite(output_image_path, processed_frame)\n",
    "\n",
    "        cv2.imshow('Detections', processed_frame)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "input_path = 'C:\\\\Users\\\\HP\\\\Desktop\\\\fall\\\\harassDataset\\\\Testing\\\\HarassamentTesting\\\\T1.mov'\n",
    "process_input(input_path, confidence_threshold=0.55)  # Confidence threshold can be adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
